{
    "collab_server" : "",
    "contents" : "Sys.info()[1:5]\nsessionInfo()\n\nlibrary('stringr')\nlibrary('knitr')\nlibrary(RCurl)\nlibrary(\"e1071\")\nx <- getURL(\"https://raw.githubusercontent.com/Abuamany/People-Analytics/master/mydata10.csv\")\ny <- read.csv(text = x)\nhead(y)\n\nset.seed(123)\n\ntal_eval<-y\n\ntal_eval<-tal_eval[,2:8]\nnv <- length(tal_eval) # number of attributes\nnv\n\ncm<-list()\n\nx<-tal_eval[,1:(nv-1)] \ny<-tal_eval[,nv]\n\nhead(y)\n\n\nfmla<-paste(colnames(tal_eval)[1:(nv-1)],collapse=\"+\")\nfmla<-paste0(colnames(tal_eval)[nv],\"~\",fmla)\nfmla<-as.formula(fmla)\n\nfmla\n\nnlev<-nlevels(y) # number of factors describing class\nnlev\n\n\n#Multinominal\nlibrary(nnet)\nlibrary(caret)\nmodel<-multinom(fmla, data = tal_eval, maxit = 500, trace=FALSE)\nprob<-predict(model,x,type=\"probs\") \npred<-apply(prob,1,which.max)\npred[which(pred==\"1\")]<-levels(y)[1] \npred[which(pred==\"2\")]<-levels(y)[2] \npred[which(pred==\"3\")]<-levels(y)[3] \npred[which(pred==\"4\")]<-levels(y)[4] \npred<-as.factor(pred)\nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\nmtab\ncm[[1]]<-c(\"Multinomial\",\"MULTINOM\",confusionMatrix(mtab))\ncm[[1]]$table\ncm[[1]]$overall[1]\n\n#Logistic Regression\nlibrary(VGAM)\nmodel<-vglm(fmla, family = \"multinomial\", data = tal_eval, maxit = 100) \nprob<-predict(model,x,type=\"response\") \npred<-apply(prob,1,which.max) \npred[which(pred==\"1\")]<-levels(y)[1] \npred[which(pred==\"2\")]<-levels(y)[2] \npred[which(pred==\"3\")]<-levels(y)[3] \npred[which(pred==\"4\")]<-levels(y)[4] \npred<-as.factor(pred)\nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[2]]<-c(\"Logistic Regression\",\"GLM\",confusionMatrix(mtab)) \ncm[[2]]$table\ncm[[2]]$overall[1]\n\n#Linear Discriminant Analysis\nlibrary(MASS) \nmodel<-lda(fmla,data=tal_eval) \npred<-predict(model,x)$class \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[3]]<-c(\"Linear Discriminant Analysis\",\"LDA\",confusionMatrix(mtab))\ncm[[3]]$table\n\ncm[[3]]$overall[1]\n\n#Non-Linear Classification\nlibrary(mda) \nmodel<-mda(fmla,data=tal_eval) \npred<-predict(model,x) \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[4]]<-c(\"Mixture Discriminant Analysis\",\"MDA\",confusionMatrix(mtab))\ncm[[4]]$table\n\ncm[[4]]$overall[1]\n\n#Regularized Discriminant Analysis\nlibrary(klaR) \nmodel<-rda(fmla,data=tal_eval,gamma = 0.05,lambda = 0.01) \npred<-predict(model,x)$class \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[5]]<-c(\"Regularized Discriminant Analysis\",\"RDA\",confusionMatrix(mtab))\ncm[[5]]$table\ncm[[5]]$overall[1]\n\n#Neural Netwrok\nlibrary(nnet)\nlibrary(devtools)\nlibrary(reshape)\nmodel<-nnet(fmla,data=tal_eval,size = 4, decay = 0.0001, maxit = 700, trace = FALSE)\n#import the function from Github\nsource_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')\nplot.nnet(model, alpha.val = 0.5, cex= 0.7, circle.col = list('lightblue', 'white'), bord.col = 'black')\npred<-predict(model,x,type=\"class\") \npred<-as.factor(pred)\nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[6]]<-c(\"Neural Network\",\"NNET\",confusionMatrix(mtab))\ncm[[6]]$table\n\ncm[[6]]$overall[1]\n\n#Flexible Discriminant Analysis\nlibrary(mda) \nmodel<-fda(fmla,data=tal_eval) \npred<-predict(model,x,type=\"class\") \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[7]]<-c(\"Flexible Discriminant Analysis\",\"FDA\",confusionMatrix(mtab))\ncm[[7]]$table\n\ncm[[7]]$overall[1]\n\n#Support Vector Machine\nlibrary(kernlab) \nmodel<-ksvm(fmla,data=tal_eval) \npred<-predict(model,x,type=\"response\") \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[8]]<-c(\"Support Vector Machine\",\"SVM\",confusionMatrix(mtab))\ncm[[8]]$table\ncm[[8]]$overall[1]\n\n#k-Nearest Neighbors\nlibrary(caret) \nmodel<-knn3(fmla,data=tal_eval,k=nlev+1) \npred<-predict(model,x,type=\"class\") \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[9]]<-c(\"k-Nearest Neighbors\",\"KNN\",confusionMatrix(mtab))\ncm[[9]]$table\ncm[[9]]$overall[1]\n\n#B8 Naive Bayes\nlibrary(e1071) \nmodel<-naiveBayes(fmla,data=tal_eval,k=nlev+1) \npred<-predict(model,x) \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[10]]<-c(\"Naive Bayes\",\"NBAYES\",confusionMatrix(mtab))\ncm[[10]]$table\ncm[[10]]$overall[1]\n\n#3.C Non-Linear Classification with Decision Trees\n#3.C1 Classification and Regression Trees(CART)\nlibrary(rpart) \nlibrary(rpart.plot)\nmodel<-rpart(fmla,data=tal_eval) \n# prp(model, faclen=3)\nrpart.plot(model)\npred<-predict(model, x ,type=\"class\") \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[11]]<-c(\"classification and Regression Trees\",\"CART\",confusionMatrix(mtab))\ncm[[11]]$table\ncm[[11]]$overall[1]\n\n#3.C2 OneR\nlibrary(RWeka)\nmodel<-OneR(fmla,data=tal_eval) \npred<-predict(model,x,type=\"class\") \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[12]]<-c(\"One R\",\"ONE-R\",confusionMatrix(mtab))\ncm[[12]]$table\ncm[[12]]$overall[1]\n\n#3.C3 C4.5\nlibrary(RWeka) \nmodel<-J48(fmla,data=tal_eval) \npred<-predict(model,x) \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[13]]<-c(\"C4.5\",\"C45\",confusionMatrix(mtab))\ncm[[13]]$table\ncm[[12]]$overall[1]\n\n#3.C4 PART\nlibrary(RWeka) \nmodel<-PART(fmla,data=tal_eval) \npred<-predict(model,x) \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[14]]<-c(\"PART\",\"PART\",confusionMatrix(mtab))\ncm[[14]]$table\ncm[[14]]$overall[1]\n\n#3.C5 Bagging CART\nlibrary(ipred) \nmodel<-bagging(fmla,data=tal_eval) \npred<-predict(model,x) \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[15]]<-c(\"Bagging CART\",\"BAG-CART\",confusionMatrix(mtab))\ncm[[15]]$table\ncm[[15]]$overall[1]\n\n#3.C6 Random Forest\nlibrary(randomForest) \nmodel<-randomForest(fmla,data=tal_eval) \npred<-predict(model,x) \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[16]]<-c(\"Random Forest\",\"RF\",confusionMatrix(mtab))\ncm[[16]]$table\n\ncm[[16]]$overall[1]\n\n#C7 Gradient Boosted Machine\nlibrary(gbm) \nmodel<-gbm(fmla,data=tal_eval,n.trees=5000,interaction.depth=nlev,shrinkage=0.001,bag.fraction=0.8,distribution=\"multinomial\",verbose=FALSE,n.cores=4) \nprob<-predict(model,x,n.trees=5000,type=\"response\")\npred<-apply(prob,1,which.max)\npred[which(pred==\"1\")]<-levels(y)[1]\npred[which(pred==\"2\")]<-levels(y)[2]\npred[which(pred==\"3\")]<-levels(y)[3]\npred[which(pred==\"4\")]<-levels(y)[4]\npred<-as.factor(pred)\nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[17]]<-c(\"Gradient Boosted Machine\",\"GBM\",confusionMatrix(mtab))\ncm[[17]]$table\ncm[[17]]$overall[1]\n\n#C8 Boosted C5.0\nlibrary(C50) \nmodel<-C5.0(fmla,data=tal_eval,trials=10) \n# tree <- rpart(model,data=tal_eval,control=rpart.control(minsplit=20,cp=0,digits=6))\n# prp(tree,faclen=3)\npred<-predict(model,x) \nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[18]]<-c(\"Boosted C5.0\",\"BOOST-C50\",confusionMatrix(mtab))\ncm[[18]]$table\ncm[[18]]$overall[1]\n\n#3.C9 JRip\nlibrary(RWeka)\nmodel<-JRip(fmla,data=tal_eval)\npred<-predict(model,x)\nl<-union(pred,y)\nmtab<-table(factor(pred,l),factor(y,l))\ncm[[19]]<-c(\"JRip\",\"JRIP\",confusionMatrix(mtab))\ncm[[19]]$table\ncm[[19]]$overall[1]\n\n#3.C10 Deep Learning\nne<-6\nlibrary(h2o)\nlocalH2O=h2o.init(nthreads=-1)\nh2o.no_progress()\ntal_eval.hex=h2o.uploadFile(path=paste0(\"/home/peopleanalytics/Desktop/tal_eval1.csv\"))\nmodel<-h2o.deeplearning(x=1:(ne-1), y=ne, training_frame = tal_eval.hex, variable_importances =TRUE)\nprob<-h2o.predict(model,tal_eval.hex)\nprob1<-as.matrix(prob[,1])\ntable(prob1)\nl<-union(prob1,y)\nmtab<-table(factor(prob1,l),factor(y,l))\ncm[[17]]<-c(\"h2o Deep learning\",\"DPL\",confusionMatrix(mtab))\ncm[[17]]$table\ncm[[17]]$overall[1]\nh2o.shutdown(prompt = FALSE)\n\n#3.C11 XGB\nset.seed(42)\nlibrary(xgboost)\nmodel_xgb <- caret::train(class ~ .,\n                          data = tal_eval,\n                          method = \"xgbTree\",\n                          preProcess = c(\"scale\", \"center\"),\n                          trControl = trainControl(method = \"repeatedcv\", \n                                                   number = 3, \n                                                   repeats = 3, \n                                                   savePredictions = TRUE, \n                                                   verboseIter = FALSE))\n#Feature Importance\nimportance <- varImp(model_xgb, scale = TRUE)\nplot(importance)\n\n\n#predicting test data\ntable(predict(model_xgb, x), y)\nconfusionMatrix(predict(model_xgb, x), y)\nresults <- data.frame(actual = y,predict(model_xgb, x, type = \"prob\"))\n\nmtab<-table(predict(model_xgb, x), y)\ncm[[20]]<-c(\"XGB\",\"XGB\",confusionMatrix(mtab))\ncm[[20]]$table\ncm[[20]]$overall[1]\n\n#Step 4. Performance Comparison\nlibrary(microbenchmark)\nmbm<-microbenchmark(\n  m1<-multinom(fmla, data = tal_eval, maxit = 500, trace=FALSE),\n  m2<-vglm(fmla, family = \"multinomial\", data = tal_eval, maxit = 100),\n  m3<-lda(fmla,data=tal_eval),\n  m4<-mda(fmla,data=tal_eval),\n  m5<-rda(fmla,data=tal_eval,gamma = 0.05,lambda = 0.01),  \n  m6<-nnet(fmla,data=tal_eval,size = 4, decay = 0.0001, maxit = 700,trace=FALSE),\n  m7<-fda(fmla,data=tal_eval),\n  m8<-ksvm(fmla,data=tal_eval), \n  m9<-knn3(fmla,data=tal_eval,k=nlev+1), \n  m10<-naiveBayes(fmla,data=tal_eval,k=nlev+1), \n  m11<-rpart(fmla,data=tal_eval), \n  m12<-OneR(fmla,data=tal_eval), \n  m13<-J48(fmla,data=tal_eval), \n  m14<-PART(fmla,data=tal_eval), \n  m15<-bagging(fmla,data=tal_eval), \n  m16<-randomForest(fmla,data=tal_eval), \n  m18<-C5.0(fmla,data=tal_eval,trials=10), \n  m19<-JRip(fmla,data=tal_eval),\n  m20<-caret::train(class ~ .,data = tal_eval,\n                       method = \"xgbTree\",\n                       preProcess = c(\"scale\", \"center\"),\n                       trControl = trainControl(method = \"repeatedcv\", \n                                                number = 3, \n                                                repeats = 3, \n                                                savePredictions = TRUE, \n                                                verboseIter = FALSE))) \n\n\n\n  \n  \n  \n  \nlibrary(dplyr)\nmodels<-length(cm)\nmbm$expr<-rep(sapply(1:models, function(i) {cm[[i]][[2]]}),5)\nmbm<-aggregate(x=mbm$time,by=list(Model=mbm$expr),FUN=mean)\nmbm$x<-mbm$x/min(mbm$x)\nresults<-sapply (1:models, function(i) {c(cm[[i]][[1]],cm[[i]][[2]],mbm$x[i],cm[[i]]$overall[1:6])})\nrow.names(results)<-c(\"Description\",\"Model\",\"Model_Time_X\",names(cm[[1]]$overall[1:6]))\nresults<-as.data.frame(t(results))\nresults[,3:9]<-sapply(3:9,function(i){results[,i]<-as.numeric(levels(results[,i])[results[,i]])})\nresults<-results[,-(8:9)]\nresults<-arrange(results,desc(Accuracy))\nresults\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nres<-data.frame(Model=results$Model,Accuracy=results$Accuracy,Speed=1/results$Model_Time_X,Overall=results$Accuracy/results$Model_Time_X)\nlibrary(RColorBrewer)\nmyPalette <- colorRampPalette(rev(brewer.pal(12, \"Set3\")))\nsc <- scale_colour_gradientn(colours = myPalette(256), limits=c(0.8, 1))\n\nkable(res)\ng<-ggplot(res,aes(x=reorder(Model,-Accuracy),y=Accuracy,fill=Model))+\n  geom_bar(stat=\"identity\")+\n  coord_polar(theta=\"x\",direction=1)+\n  labs(x=\"Machine Learning Model\",y=\"Prediction Accuracy\")+\n  theme(legend.position=\"bottom\",legend.box=\"horizontal\")+\n  ggtitle('Car Evaluation Dataset Accuracy Performance')\ng\n\n\n\n",
    "created" : 1492673784037.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1117176475",
    "id" : "1D8BB9D4",
    "lastKnownWriteTime" : 1492693490,
    "last_content_update" : 1492694248394,
    "path" : "~/Desktop/20 ML-Talent.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}